@inproceedings{10.1145/3447818.3461472,
author = {Selvitopi, Oguz and Brock, Benjamin and Nisa, Israt and Tripathy, Alok and Yelick, Katherine and Bulu\c{c}, Ayd\i{}n},
title = {Distributed-Memory Parallel Algorithms for Sparse Times Tall-Skinny-Dense Matrix Multiplication},
year = {2021},
isbn = {9781450383356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447818.3461472},
doi = {10.1145/3447818.3461472},
abstract = {Sparse times dense matrix multiplication (SpMM) finds its applications in well-established fields such as computational linear algebra as well as emerging fields such as graph neural networks. In this study, we evaluate the performance of various techniques for performing SpMM as a distributed computation across many nodes by focusing on GPU accelerators. We examine how the actual local computational performance of state-of-the-art SpMM implementations affect computational efficiency as dimensions change when we scale to large numbers of nodes, which proves to be an unexpectedly important bottleneck. We also consider various distribution strategies, including A-Stationary, B-Stationary, and C-Stationary algorithms, 1.5D and 2D algorithms, and RDMA-based and bulk synchronous methods of data transfer. Our results show that the best choice of algorithm and implementation technique depends not only on the cost of communication for particular matrix sizes and dimensions, but also on the performance of local SpMM operations. Our evaluations reveal that with the involvement of GPU accelerators, the best design choices for SpMM differ from the conventional algorithms that are known to perform well for dense matrix-matrix or sparse matrix-sparse matrix multiplies.},
booktitle = {Proceedings of the ACM International Conference on Supercomputing},
pages = {431â€“442},
numpages = {12},
keywords = {parallel algorithms, RDMA, graphics accelerators, sparse matrices, sparse linear algebra},
location = {Virtual Event, USA},
series = {ICS '21}
}

@inproceedings{10.1145/3149704.3149767,
author = {Flegar, Goran and Anzt, Hartwig},
title = {Overcoming Load Imbalance for Irregular Sparse Matrices},
year = {2017},
isbn = {9781450351362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3149704.3149767},
doi = {10.1145/3149704.3149767},
abstract = {In this paper we propose a load-balanced GPU kernel for computing the sparse matrix vector (SpMV) product. Making heavy use of the latest GPU programming features, we also enable satisfying performance for irregular and unbalanced matrices. In a performance comparison using 400 test matrices we reveal the new kernel being superior to the most popular SpMV implementations.},
booktitle = {Proceedings of the Seventh Workshop on Irregular Applications: Architectures and Algorithms},
articleno = {2},
numpages = {8},
keywords = {Load Balancing, Irregular access pattern, GPU, Sparse Matrix Vector Product},
location = {Denver, CO, USA},
series = {IA3'17}
}