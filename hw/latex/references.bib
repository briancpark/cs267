@inproceedings{10.1145/3447818.3461472,
author = {Selvitopi, Oguz and Brock, Benjamin and Nisa, Israt and Tripathy, Alok and Yelick, Katherine and Bulu\c{c}, Ayd\i{}n},
title = {Distributed-Memory Parallel Algorithms for Sparse Times Tall-Skinny-Dense Matrix Multiplication},
year = {2021},
isbn = {9781450383356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447818.3461472},
doi = {10.1145/3447818.3461472},
abstract = {Sparse times dense matrix multiplication (SpMM) finds its applications in well-established fields such as computational linear algebra as well as emerging fields such as graph neural networks. In this study, we evaluate the performance of various techniques for performing SpMM as a distributed computation across many nodes by focusing on GPU accelerators. We examine how the actual local computational performance of state-of-the-art SpMM implementations affect computational efficiency as dimensions change when we scale to large numbers of nodes, which proves to be an unexpectedly important bottleneck. We also consider various distribution strategies, including A-Stationary, B-Stationary, and C-Stationary algorithms, 1.5D and 2D algorithms, and RDMA-based and bulk synchronous methods of data transfer. Our results show that the best choice of algorithm and implementation technique depends not only on the cost of communication for particular matrix sizes and dimensions, but also on the performance of local SpMM operations. Our evaluations reveal that with the involvement of GPU accelerators, the best design choices for SpMM differ from the conventional algorithms that are known to perform well for dense matrix-matrix or sparse matrix-sparse matrix multiplies.},
booktitle = {Proceedings of the ACM International Conference on Supercomputing},
pages = {431–442},
numpages = {12},
keywords = {parallel algorithms, RDMA, graphics accelerators, sparse matrices, sparse linear algebra},
location = {Virtual Event, USA},
series = {ICS '21}
}

@inproceedings{10.1145/3149704.3149767,
author = {Flegar, Goran and Anzt, Hartwig},
title = {Overcoming Load Imbalance for Irregular Sparse Matrices},
year = {2017},
isbn = {9781450351362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3149704.3149767},
doi = {10.1145/3149704.3149767},
abstract = {In this paper we propose a load-balanced GPU kernel for computing the sparse matrix vector (SpMV) product. Making heavy use of the latest GPU programming features, we also enable satisfying performance for irregular and unbalanced matrices. In a performance comparison using 400 test matrices we reveal the new kernel being superior to the most popular SpMV implementations.},
booktitle = {Proceedings of the Seventh Workshop on Irregular Applications: Architectures and Algorithms},
articleno = {2},
numpages = {8},
keywords = {Load Balancing, Irregular access pattern, GPU, Sparse Matrix Vector Product},
location = {Denver, CO, USA},
series = {IA3'17}
}
@article{10.1007/s10586-018-2810-y,
author = {Lim, Roktaek and Lee, Yeongha and Kim, Raehyun and Choi, Jaeyoung},
title = {An Implementation of Matrix---Matrix Multiplication on the Intel KNL Processor with AVX-512},
year = {2018},
issue_date = {December  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-018-2810-y},
doi = {10.1007/s10586-018-2810-y},
abstract = {The second generation Intel Xeon Phi processor codenamed Knights Landing (KNL) have recently emerged with 2D tile mesh architecture and the Intel AVX-512 instructions. However, it is very difficult for general users to get the maximum performance from the new architecture since they are not familiar with optimal cache reuse, efficient vectorization, and assembly language. In this paper, we illustrate several developing strategies to achieve good performance with C programming language by carrying out general matrix---matrix multiplications and without the use of assembly language. Our implementation of matrix---matrix multiplication is based on blocked matrix multiplication as an optimization technique that improves data reuse. We use data prefetching, loop unrolling, and the Intel AVX-512 to optimize the blocked matrix multiplications. When we use a single core of the KNL, our implementation achieves up to 98% of SGEMM and 99% of DGEMM using the Intel MKL, which is the current state-of-the-art library. Our implementation of the parallel DGEMM using all 68 cores of the KNL achieves up to 90% of DGEMM using the Intel MKL.},
journal = {Cluster Computing},
month = {dec},
pages = {1785–1795},
numpages = {11},
keywords = {Knights Landing, AVX-512, Vectorization, Threading, Matrix---matrix multiplication}
}
@article{10.1145/1356052.1356053,
author = {Goto, Kazushige and Geijn, Robert A. van de},
title = {Anatomy of High-Performance Matrix Multiplication},
year = {2008},
issue_date = {May 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {3},
issn = {0098-3500},
url = {https://doi.org/10.1145/1356052.1356053},
doi = {10.1145/1356052.1356053},
abstract = {We present the basic principles that underlie the high-performance implementation of the matrix-matrix multiplication that is part of the widely used GotoBLAS library. Design decisions are justified by successively refining a model of architectures with multilevel memories. A simple but effective algorithm for executing this operation results. Implementations on a broad selection of architectures are shown to achieve near-peak performance.},
journal = {ACM Trans. Math. Softw.},
month = {may},
articleno = {12},
numpages = {25},
keywords = {Linear algebra, matrix multiplication, basic linear algebra subprogrms}
}

@article{DBLP:journals/corr/abs-1712-05889,
  author    = {Philipp Moritz and
               Robert Nishihara and
               Stephanie Wang and
               Alexey Tumanov and
               Richard Liaw and
               Eric Liang and
               William Paul and
               Michael I. Jordan and
               Ion Stoica},
  title     = {Ray: {A} Distributed Framework for Emerging {AI} Applications},
  journal   = {CoRR},
  volume    = {abs/1712.05889},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.05889},
  eprinttype = {arXiv},
  eprint    = {1712.05889},
  timestamp = {Mon, 13 Aug 2018 16:46:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1712-05889.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{cupy_learningsys2017,
  author       = "Okuta, Ryosuke and Unno, Yuya and Nishino, Daisuke and Hido, Shohei and Loomis, Crissman",
  title        = "CuPy: A NumPy-Compatible Library for NVIDIA GPU Calculations",
  booktitle    = "Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Thirty-first Annual Conference on Neural Information Processing Systems (NIPS)",
  year         = "2017",
  url          = "http://learningsys.org/nips17/assets/papers/paper_16.pdf"
}
@book{10.5555/265932,
author = {Blackford, L. S. and Choi, J. and Cleary, A. and D'Azeuedo, E. and Demmel, J. and Dhillon, I. and Hammarling, S. and Henry, G. and Petitet, A. and Stanley, K. and Walker, D. and Whaley, R. C. and Dongarra, Jack J.},
title = {ScaLAPACK User's Guide},
year = {1997},
isbn = {0898713978},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA}
}
@misc{NumS,
  title = {NumS, A library that translates Python and NumPy to optimized distributed systems code.},
  howpublished = {\url{https://github.com/nums-project/nums}}
}
@misc{Ray,
    title = {Ray: A universal framework for distributed computing}, 
    author = {I. Stoica},
    howpublished = {\url{https://drive.google.com/file/d/1lWi4_vYvMcJJ6bEARvPEigad4WEdwPtq/view}}
}